{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning of the Gemma-2b Model for Data Science Question Generation\n",
    "\n",
    "**Abstract:** This project details the fine-tuning of the Gemma-2b language model. The primary objective is to enhance the model's capability to generate relevant and insightful questions within the domain of data science and machine learning.\n",
    "\n",
    "**Model Selection:** The Gemma-2b model was selected for this task due to its efficient architecture and powerful performance. Its relatively small size makes it a suitable candidate for fine-tuning on a custom dataset, offering a balance between computational cost and expected performance for this specific application.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "### 1.1. Dependency Installation\n",
    "\n",
    "The following libraries are required for the experiment and are installed in this section:\n",
    "\n",
    "* `bitsandbytes`: For model quantization, which reduces the model's memory footprint and computational demand.\n",
    "* `trl`: The Transformer Reinforcement Learning library, utilized for its `SFTTrainer` to facilitate supervised fine-tuning.\n",
    "* `tensorboard`: To monitor and visualize the training process and model performance metrics.\n",
    "* `jupyter_tensorboard`: To integrate Tensorboard with the Jupyter environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bitsandbytes --upgrade --no-cache-dir\n",
    "%pip install trl\n",
    "%pip install tensorboard\n",
    "%pip install jupyter_tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Hugging Face Authentication\n",
    "\n",
    "Authentication with Hugging Face is a prerequisite for accessing the Gemma-2b model, as it is hosted in a private repository. This step involves using a generated access token to log in. For detailed instructions on token generation, please refer to the [Hugging Face documentation](https://huggingface.co/docs/hub/en/security-tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = \"hf_token\"\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Configuration and Loading\n",
    "\n",
    "### 2.1. Quantization Configuration\n",
    "\n",
    "To optimize the model for training, we employ 4-bit quantization using the `bitsandbytes` library. The specific configurations are as follows:\n",
    "\n",
    "* **`load_in_4bit=True`**: This enables the loading of the model with 4-bit precision.\n",
    "* **`bnb_4bit_use_double_quant=True`**: Double quantization is applied for enhanced memory efficiency.\n",
    "* **`bnb_4bit_quant_type=\"nf4\"`**: The \"Normal Float 4\" (nf4) quantization type is used, which is a 4-bit data type optimized for normally distributed weights.\n",
    "* **`bnb_4bit_compute_dtype=torch.bfloat16`**: The computation is performed using the `bfloat16` data type for a balance of precision and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "D5FPoHe1Rf37"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Model and Tokenizer Loading\n",
    "\n",
    "The pre-trained Gemma-2b model and its corresponding tokenizer are loaded from the Hugging Face model hub. The previously defined quantization configuration is applied during this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2b-it\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\":0}\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\", add_eos_token=True)\n",
    "\n",
    "input_text = \"Ask a question about Overfitting/Underfitting.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=32)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "### 3.1. Dataset Loading and Preprocessing\n",
    "\n",
    "The dataset for fine-tuning is loaded from a JSON file. A preprocessing step is performed to reformat the data into a `prompt` and `completion` structure. This standardized format is required by the `SFTTrainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P50p8QuDzw6J",
    "outputId": "86324fb2-9d57-41f3-8c4b-1c6408e7080d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "{\n",
      "  \"prompt\": \"Act as a machine learning interviewer and formulate a question on Overfitting/Underfitting.\",\n",
      "  \"completion\": \"What is the role of complexity in model performance?\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "dataset = 'assets/dataset/dataset_5k.json'\n",
    "\n",
    "with open(dataset, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "converted_data = []\n",
    "\n",
    "for item in data:\n",
    "    converted_item = {\n",
    "v\n",
    "    }\n",
    "    converted_data.append(converted_item)\n",
    "\n",
    "print(\"Example:\")\n",
    "print(json.dumps(converted_data[0], indent=2, ensure_ascii=False))\n",
    "\n",
    "output_file = '/content/dataset_2k_prompt_completion.json'\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(converted_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "main_df = pd.read_json('/content/dataset_2k_prompt_completion.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Dataset Splitting\n",
    "\n",
    "The preprocessed dataset is split into three subsets:\n",
    "\n",
    "* **Training Set:** Used to train the model.\n",
    "* **Validation Set:** Used to evaluate the model's performance during training and to tune hyperparameters.\n",
    "* **Test Set:** Reserved for the final evaluation of the model's performance after training is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "wANrW5FgzyPs"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, temp_data = train_test_split(main_df, test_size=0.2, random_state=42)\n",
    "\n",
    "eval_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training\n",
    "\n",
    "### 4.1. Preparing the Model for K-bit Training\n",
    "\n",
    "The model is prepared for k-bit training using the `peft` (Parameter-Efficient Fine-Tuning) library. This step enables gradient checkpointing to reduce memory usage during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HWLZCTS5zz5b"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel, prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Identifying Target Layers for LoRA\n",
    "\n",
    "For the LoRA (Low-Rank Adaptation) fine-tuning technique, we need to identify the specific layers of the model that will be adapted. This typically involves targeting the linear (fully connected) layers of the transformer architecture. The following code identifies all 4-bit linear modules in the model that will be targeted by LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Etuz2pi0jhE",
    "outputId": "0a7e29bd-73d4-4da0-9751-8515431b7f94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['down_proj', 'gate_proj', 'o_proj', 'v_proj', 'k_proj', 'up_proj', 'q_proj']\n"
     ]
    }
   ],
   "source": [
    "import bitsandbytes as bnb\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "        if 'lm_head' in lora_module_names:\n",
    "            lora_module_names.remove('lm_head')\n",
    "\n",
    "    return list(lora_module_names)\n",
    "\n",
    "modules = find_all_linear_names(model)\n",
    "print(modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. LoRA Configuration\n",
    "\n",
    "The LoRA configuration is defined with the following parameters:\n",
    "\n",
    "* **`r` (Rank):** 64\n",
    "* **`lora_alpha` (Alpha):** 32\n",
    "* **`target_modules`:** The list of linear layers identified in the previous step.\n",
    "* **`lora_dropout`:** 0.05\n",
    "* **`bias`:** \"none\"\n",
    "* **`task_type`:** \"CAUSAL_LM\"\n",
    "\n",
    "This configuration is then applied to the model using `get_peft_model` from the `peft` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "uEjBKZk5z0jI"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=32,\n",
    "    target_modules=modules,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Verifying Trainable Parameters\n",
    "\n",
    "After applying LoRA, we can verify the number of trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y3NnrsRVREiN",
    "outputId": "ba80fe0b-0a60-479f-aa52-b1493d87daaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 78446592 | total: 2584619008 | Percentage: 3.0351%\n"
     ]
    }
   ],
   "source": [
    "trainable, total = model.get_nb_trainable_parameters()\n",
    "print(f'Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Training Arguments and Trainer Initialization\n",
    "\n",
    "The training arguments are defined using the `TrainingArguments` class from the `transformers` library. These arguments specify various training parameters such as batch size, learning rate, and logging strategy. The `SFTTrainer` is then initialized with the model, datasets, LoRA configuration, and training arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "import transformers\n",
    "from transformers import TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "eval_dataset = Dataset.from_pandas(temp_data)\n",
    "\n",
    "print(train_data.columns)\n",
    "\n",
    "print('prompt' in train_data.columns)\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"gemma-2b-FT_results\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=250,\n",
    "    fp16=True,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=lora_config,\n",
    "    args=training_arguments\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. Model Training\n",
    "\n",
    "The training process is initiated by calling the `train()` method on the `trainer` object. The training progress, including training and validation loss, is logged to Tensorboard.\n",
    "\n",
    "- You can view training logs in colab with tensorboard as follows:\n",
    "\n",
    "    - %load_ext tensorboard\n",
    "\n",
    "    - %tensorboard --logdir ./logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "xLPFfTioYwQh"
   },
   "outputs": [],
   "source": [
    "new_model = \"Gemma-2b_interview-FT\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Saving and Merging\n",
    "\n",
    "### 5.1. Saving the Fine-Tuned Model\n",
    "\n",
    "The fine-tuned model adapters are saved to the specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "8WGKcE06ZAA6"
   },
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CmrXAjS0ZKtm",
    "outputId": "fa1ae173-bfee-4088-a672-b643aeee6266"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1517"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WwjXJu2nZRCY",
    "outputId": "12fcfbf1-9a54-4cd9-b011-4e3e0efa935c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Merging the Model with Base Model\n",
    "\n",
    "The fine-tuned LoRA adapters are merged with the base Gemma-2b model to create a standalone, fine-tuned model. The merged model is then saved to a new directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2b-it\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "\n",
    "merged_model= PeftModel.from_pretrained(base_model, new_model)\n",
    "merged_model= merged_model.merge_and_unload()\n",
    "\n",
    "merged_model.save_pretrained(\"merged_model\", safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"merged_model\")\n",
    "tokenizer.padding_side= \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "\n",
    "A simple test is conducted to evaluate the performance of the fine-tuned model on a sample prompt. This provides a qualitative assessment of the model's ability to generate relevant questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "2dxDG-qfDgv4"
   },
   "outputs": [],
   "source": [
    "def get_completion(query:str, model, tokenizer) -> str:\n",
    "  device = \"cuda:0\"\n",
    "\n",
    "  prompt_template = \"\"\"\n",
    "  <start_of_turn>\n",
    "   user\n",
    "   {query}\n",
    "   <end_of_turn>\\n\n",
    "   <start_of_turn>model\n",
    "  \"\"\"\n",
    "\n",
    "  prompt = prompt_template.format(query=query)\n",
    "\n",
    "  encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "  model_inputs = encodeds.to(device)\n",
    "\n",
    "  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "  decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "  return(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5vB3SD6RamEY",
    "outputId": "916d40a2-9808-4a1d-d0f9-77da8ac598e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  \n",
      "   user\n",
      "   Ask a question about Overfitting/Underfitting.\n",
      "   \n",
      "\n",
      "   model\n",
      "  What causes overfitting and how can it be prevented?\n"
     ]
    }
   ],
   "source": [
    "result = get_completion(query=\"Ask a question about Overfitting/Underfitting.\", model=merged_model, tokenizer=tokenizer)\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
